{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissecting the transcriptional effects of somatic mutations in human cancers\n",
    "### *** Andrea Garofoli***\n",
    "\n",
    "## Original Full Project's Abstract\n",
    "*Large-scale sequencing studies in human cancers have uncovered hundreds of thousands of novel somatic mutations, only some of which are likely to confer significant advantage to cancer cells. As exhaustive functional characterization of each novel mutation is unfeasible, distinguishing the ‘driver’/targetable mutations from the ‘passenger’ mutations presents significant clinical challenges. With the major highly recurrent mutations already characterized as oncogenic, I hypothesize that the pathogenicity and/or actionability of uncharacterized mutations could be predicted based on whether these mutations lead to transcriptomic changes that resemble those of known activating mutations. Here I aim 1) to generate gene signatures for known activating mutations in oncogenes and 2) to infer the pathogenicity of uncharacterized mutations and to functionally benchmark the predictions. To achieve these aims, I will first define a gene signature for each known activating mutation and validate the gene signatures in an independent cohort. I will then predict the pathogenicity of the uncharacterized mutations and benchmark the predictions by testing the oncogenic potential of a subset of mutations and their response to targeted agents using in vitro and/or in vivo models. I anticipate that the results will serve as an extensive and valuable resource and will provide a computational framework to predict the significance of novel mutations for the scientific and clinical communities.*\n",
    "\n",
    "\n",
    "## Python Project's Introduction\n",
    "\n",
    "This python project's aim is to perform the first phase of this study.\n",
    "The data I've worked comes from the **METABRIC** cohort (*Nature 2012 & Nat. Commun. 2016*) and was previously collected and reviewed by my supervisor. \n",
    "These first tests were performed on the *PIK3CA* mutation. The many *in vivo* and *in vitro* information that could be easily found in the literature made it easy for us to see if the approach actually works.\n",
    "\n",
    "There are 4 main phases:\n",
    "+ Sampe Selection\n",
    "+ Feature Selection\n",
    "+ Model Building and Validation\n",
    "+ Infer mutations with similiar patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Files\n",
    "\n",
    "+\t**Normalized Gene Expression** tables for the *Training* and the *Validation* sets. \n",
    "+ + *TrainingTable*’s size: 48803 rows (genes) and 960 columns (samples).\n",
    "+ +\t*ValidationTable*’s size: 48803 rows and 949 columns.\n",
    "\n",
    "+ One table with **mutations** related information (such as chromosome, locus, gene name, codon, etc.) for each sample in both datasets.\n",
    "\n",
    "+ One table with **Copy Number Alterations** related information for each sample in both datasets.\n",
    "\n",
    "+ One table with **Histopathological Features** (such as age, er and her2 status, tumor size, etc.) specifics for each sample in both datasets.\n",
    "\n",
    "All the files are the result of an internal pipeline, so they have a fixed structure and fixed names, this script was built with that in mind.\n",
    "The Gene expression tables are tab separated and stored in .txt files, the other tables are comma separated and stored in .csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "The script can return the following files and information:\n",
    "+ Two tables with the data subsets that will be used to build the machine learning model (more info in the next paragraph)\n",
    "+ Two tables with the data subsets that will be used to obtain the list of oncogenic *PIK3CA* mutations\n",
    "+ The list of genes that have the most different gene expression between the breast cancer subtype we are studying\n",
    "+ The list of uncharacterized and potentially oncogenic mutations in *PIK3CA*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Selection \n",
    "\n",
    "Here I classify the samples in two groups, the **Mutant** (samples with a mutation in *PIK3CA*) and the **Control** one (so the samples with a wt *PIK3CA*). Furthermore, I will identify and remove 2 subgroups from the Mutant sample lists by finding samples with potentially confounding mutations and histopathological features (for examples, the activation of the *PI3K* pathway may be the effect of a *PIK3CA* mutation but also the result of a mutation in *PTEN*, *AKT1* and many others) and samples with a *PIK3CA* mutation in different places than the *H1047R*, *E542K* and *E545K* hotspots. In a later phase the classification model will be used on this latter subgroup to infer potentially pathogenic/actionable mutations.\n",
    "\n",
    "The four types of samples are associated to four different colors:\n",
    "+ ***Red*** = **Mutant** samples\n",
    "+ ***Grey*** = **Control** samples\n",
    "+ ***Purple*** = Samples removed due to the presence of potentially confounding mutations or histopoathological features.\n",
    "+ ***Blue*** = Samples with *PIK3CA* mutations in different places than the 3 hotspots\n",
    "\n",
    "Comments on the implementation can be found within the code blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the Working Folder\n",
      "The working folder is /Users/andrea/Documents/BI/BI/B/PhD/PythonProject/PythonProject\n",
      "/Users/andrea/Documents/BI/BI/B/PhD/PythonProject/PythonProject\n",
      "Preparing lists\n",
      "Preparing the Training dataset\n",
      "Number of samples for each ER status:\n",
      " ER +  495 ER -  465\n",
      "Samples processed\n",
      "0/48805\n",
      "10000/48805\n",
      "20000/48805\n",
      "30000/48805\n",
      "40000/48805\n",
      "Number of samples for each subset:\n",
      " Gray  164 Purple  152 Red  179\n",
      "Samples processed\n",
      "0/48805\n",
      "10000/48805\n",
      "20000/48805\n",
      "30000/48805\n",
      "40000/48805\n",
      "Number of samples for each subset:\n",
      " Gray and Red  306 Blue  37\n",
      "Samples processed\n",
      "0/48805\n",
      "10000/48805\n",
      "20000/48805\n",
      "30000/48805\n",
      "40000/48805\n",
      "Number of samples for each subset:\n",
      " Gray  127 Red  179\n",
      "Training dataset complete\n",
      "Press enter to continue\n",
      "/Users/andrea/Documents/BI/BI/B/PhD/PythonProject/PythonProject\n",
      "Preparing lists\n",
      "Preparing the Validation dataset\n",
      "Number of samples for each ER status:\n",
      " ER +  435 ER -  514\n",
      "Samples processed\n",
      "0/48805\n",
      "10000/48805\n",
      "20000/48805\n",
      "30000/48805\n",
      "40000/48805\n",
      "Number of samples for each subset:\n",
      " Gray  139 Purple  127 Red  169\n",
      "Samples processed\n",
      "0/48805\n",
      "10000/48805\n",
      "20000/48805\n",
      "30000/48805\n",
      "40000/48805\n",
      "Number of samples for each subset:\n",
      " Gray and Red  276 Blue  32\n",
      "Samples processed\n",
      "0/48805\n",
      "10000/48805\n",
      "20000/48805\n",
      "30000/48805\n",
      "40000/48805\n",
      "Number of samples for each subset:\n",
      " Gray  107 Red  169\n",
      "Validation dataset complete\n",
      "Press enter to continue\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "from tkinter import messagebox\n",
    "import glob, os\n",
    "import re    \n",
    "\n",
    "\n",
    "####\n",
    "##   ChooseFolder\n",
    "## This function was made to made it possible for the user to select\n",
    "## the path for the folder with the Input files.\n",
    "## The function will also check for the presence of eventual output files\n",
    "## already made in a previous run of the script (which has to be removed manually\n",
    "## by the user).\n",
    "##\n",
    "####\n",
    "\n",
    "\n",
    "def ChooseFolder():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.update()\n",
    "    print(\"Select the Working Folder\")\n",
    "    dirpath = filedialog.askdirectory(initialdir=os.getcwd(),\n",
    "                                       title=\"Select the Working Folder\")\n",
    "    root.update()\n",
    "    root.destroy()\n",
    "    \n",
    "    \n",
    "    if not len(glob.glob(dirpath+\"/*.txt\"))==2 and not len(glob.glob(filename+\"/*.csv\"))==3:\n",
    "        messagebox.showinfo(\"Visualizer error\", \"Wrong number of files, maybe you didn't choose the right directory?\")\n",
    "        sys.exit(\"Error, wrong file or directory\")\n",
    "    if not len(glob.glob(dirpath+\"/temp/*\"))==0:\n",
    "        messagebox.showinfo(\"Visualizer error\", \"Folder not empty, cannot be used for the temp files\")\n",
    "        sys.exit(\"Error, wrong file or directory\")\n",
    "    if not len(glob.glob(dirpath+\"/out/*\"))==0:\n",
    "        messagebox.showinfo(\"Visualizer error\", \"Folder not empty, cannot be used for the output\")\n",
    "        sys.exit(\"Error, wrong file or directory\")        \n",
    "    return(dirpath)\n",
    "\n",
    "\n",
    "####\n",
    "##   FiltList\n",
    "## This function will read the input files (see the documentation for more info about them)\n",
    "## and make a list of the samples that need to be filtered\n",
    "## from the datase.\n",
    "## The function will check for the presence of said input files.\n",
    "##\n",
    "####\n",
    "\n",
    "def FiltList(tabfo):\n",
    "    print(tabfo)            \n",
    "\n",
    "    try:\n",
    "        muta = open(tabfo +'/mut.csv',\"r\").read().split('\\n')\n",
    "    except FileNotFoundError:\n",
    "        sys.exit(\"Mutation file cannot be found, was it renamed or moved?\")\n",
    "\n",
    "\n",
    "    print(\"Preparing lists\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        m = open(tabfo +'/pheno_r_file.csv',\"r\").read().split('\\n')\n",
    "    except FileNotFoundError:\n",
    "        sys.exit(\"Phenotype file cannot be found, was it renamed or moved?\")\n",
    "    \n",
    "    \n",
    "    phen=[]\n",
    "\n",
    "# Histopathological features used to filter the samples: ER HER2 status and histology    \n",
    "    \n",
    "    for n in range(len(m)-1):\n",
    "        x=m[n].split(\",\")\n",
    "        for i in range(len(x)):\n",
    "            if x[3]==\"POS\" and x[4]==\"NEG\":\n",
    "                if x[10]==\"Ductal/NST\" :\n",
    "                    phen.append(x[1])\n",
    "    \n",
    "    phen=sorted(set(phen))\n",
    "\n",
    "# This is the list of the potentially confounding mutations\n",
    "\n",
    "    lismut=[\"AKT1\", \"AKT2\", \"AKT3\", \"CDKN1A\", \"CDKN1B\", \"ERBB2\", \"ERBB3\", \"ERBB4\", \"FGFR1\", \"FGFR2\", \"FGFR3\",\n",
    "#            \"FGFR4\", \"GRB2\", \"HIF1A\", \"HRAS\", \"IGF1R\", \"IRS1\", \"KIT\", \"MTOR\", \"NRAS\", \"PDGFRA\", \"PDGFRB\", \n",
    "#            \"PIK3CB\", \"PIK3R1\", \"PRKCA\", \"PRKCB\", \"PRKCG\", \"PTEN\", \"RICTOR\", \"RPS6KB1\", \"RPTOR\", \"SOS1\", \n",
    "#            \"TSC1\", \"TSC2\"]\n",
    "#    lismut=[\"AKT1\", \"AKT2\", \"AKT3\", \"CDKN1A\", \"CDKN1B\", \"PIK3CA\", \"ERBB3\", \"ERBB4\", \"FGFR1\", \"FGFR2\", \"FGFR3\",\n",
    "            \"FGFR4\", \"GRB2\", \"HIF1A\", \"HRAS\", \"IGF1R\", \"IRS1\", \"KIT\", \"MTOR\", \"NRAS\", \"PDGFRA\", \"PDGFRB\", \n",
    "            \"PIK3CB\", \"PIK3R1\", \"PRKCA\", \"PRKCB\", \"PRKCG\", \"PTEN\", \"RICTOR\", \"RPS6KB1\", \"RPTOR\", \"SOS1\", \n",
    "            \"TSC1\", \"TSC2\"]\n",
    "\n",
    "    lmu=[]\n",
    "\n",
    "    for n in range (1, len(muta)-1):\n",
    "        x = muta[n].split(',')\n",
    "        x[9]=x[9]\n",
    "        if x[9] in lismut  :\n",
    "            lmu.append(x[1])\n",
    "\n",
    "    lmu=sorted(set(lmu))\n",
    "    \n",
    "    Hmu=[]\n",
    "    blmu=[]\n",
    "\n",
    "# Here the script separates the PIK3CA mutants into the Red and Blue subsets    \n",
    "    \n",
    "    for n in range (1, len(muta)-1):\n",
    "        x = muta[n].split(',')\n",
    "        x[9]=x[9]\n",
    "        if x[9]==\"PIK3CA\":\n",
    "#        if x[9]==\"ERBB2\":            \n",
    "            if x[12] == \"1047\" or x[12] == \"545\" or x[12] == \"542\":\n",
    "#            if x[12] == \"755\" or x[12] == \"777\" or x[12] == \"769\":\n",
    "                Hmu.append(x[1])\n",
    "            else:\n",
    "                blmu.append(x[1])\n",
    "\n",
    "    Hmu=sorted(set(Hmu))\n",
    "\n",
    "    blmu=sorted(set(blmu))\n",
    "\n",
    "\n",
    "    try:\n",
    "        p = open(tabfo +\"/CN.csv\",\"r\").read().split('\\n')\n",
    "    except FileNotFoundError:\n",
    "        sys.exit(\"CopyNumber file cannot be found, was it renamed or moved?\")\n",
    "        \n",
    "    li=[]\n",
    "    lik=[]\n",
    "\n",
    "    pa=p[0].split(',')\n",
    "\n",
    "    for i in range(1, len(p)-1):\n",
    "        x=p[i].split(',')\n",
    "        if x[7] in lismut:\n",
    "            for n in range(7,len(x)):\n",
    "                if x[n]==\"-2\" or x[n]==\"2\":\n",
    "                    li.append(pa[n])\n",
    "                else: lik.append(pa[n])\n",
    "\n",
    "    lif=sorted(set(li))\n",
    "    \n",
    "    return(phen, lmu, Hmu, blmu, lif)\n",
    "\n",
    "\n",
    "####\n",
    "##   Filt\n",
    "## This function will create the Mutation and Validation dataset\n",
    "## (see the documentation for more info about them).\n",
    "## The function will also print on screen some additional info about the\n",
    "## filtering phase such as the number of samples found in for each criteria\n",
    "## (for example ER+ and ER-) and the state of progress of each step\n",
    "## (in the form of \"processed rows / total rows\").\n",
    "##\n",
    "####\n",
    "\n",
    "\n",
    "    \n",
    "def Filt(er, mu, hm, bl, cn, file, tabfo):               \n",
    "\n",
    "    try:\n",
    "        di = open(file,\"r\").read().split('\\n')\n",
    "    except FileNotFoundError:\n",
    "        sys.exit(\"Gene expression file cannot be found, was it renamed or moved?\")\n",
    "        \n",
    "    fname=re.split(r'[\\.-\\/]',file)[-2]\n",
    "    \n",
    "    print(\"Preparing the \" + fname + \" dataset\")\n",
    "    \n",
    "    der=[\"\"]*len(di)\n",
    "\n",
    "    paz=di[0].split(',')\n",
    "\n",
    "\n",
    "    c=[0]*len(paz)  \n",
    "\n",
    "    for i in range(1, len(paz)):\n",
    "        if  paz[i] not in er:\n",
    "            c[i]= 1\n",
    "\n",
    "\n",
    "    print(\"Number of samples for each ER status:\\n ER + \",c.count(0)-1,\"ER - \", c.count(1))\n",
    "\n",
    "\n",
    "    print(\"Samples processed\")\n",
    "\n",
    "    for n in range(len(di)-1):\n",
    "        if n % 10000 == 0 : print(str(n) + \"/\" + str(len(di)))    \n",
    "        x=di[n].split(\",\")\n",
    "        der[n]=x[0]\n",
    "        for i in range(1, len(c)):\n",
    "            if c[i] == 0 :\n",
    "                der[n]=der[n]+ \",\" + x[i] \n",
    "\n",
    "\n",
    "    di= der\n",
    "    der=[\"\"]*len(di)\n",
    "\n",
    "    paz=di[0].split(',')\n",
    "\n",
    "\n",
    "    c=[0]*len(paz)\n",
    "\n",
    "\n",
    "    for i in range(1, len(paz)):\n",
    "        if  paz[i] in hm :\n",
    "            c[i]= 2\n",
    "        elif paz[i] in mu or paz[i] in cn :\n",
    "            c[i]= 1\n",
    "\n",
    "            \n",
    "    print(\"Number of samples for each subset:\\n Gray \", c.count(0)-1,\"Purple \", c.count(1),\"Red \", c.count(2))\n",
    "\n",
    "    print(\"Samples processed\")\n",
    "    \n",
    "    for n in range(len(di)-1):\n",
    "        if n % 10000 == 0 : print(str(n) + \"/\" + str(len(di)))    \n",
    "        x=di[n].split(\",\")\n",
    "        der[n]=x[0]\n",
    "        for i in range(1, len(c)):\n",
    "            if c[i] != 1 :\n",
    "                der[n]=der[n]+ \",\" + x[i]\n",
    "\n",
    "\n",
    "\n",
    "    di= der\n",
    "    der=[\"\"]*len(di)\n",
    "\n",
    "    paz=di[0].split(',')\n",
    "\n",
    "    c=[0]*len(paz) \n",
    "\n",
    "\n",
    "    for i in range(len(paz)):\n",
    "        if  paz[i] in bl and paz[i] not in hm:\n",
    "            c[i]= 1\n",
    "\n",
    "\n",
    "    print(\"Number of samples for each subset:\\n Gray and Red \", c.count(0)-1,\"Blue \", c.count(1))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Samples processed\")\n",
    "\n",
    "    for n in range(len(di)-1):\n",
    "        if n % 10000 == 0 : print(str(n) + \"/\" + str(len(di)))    \n",
    "        x=di[n].split(\",\")\n",
    "        der[n]=x[0]\n",
    "        for i in range( len(c)):\n",
    "            if c[i] == 0 and i!=0 :\n",
    "                der[n]=der[n]+ \",\" + x[i]\n",
    "            elif c[i] == 1 or i==0:\n",
    "                if i == 0:\n",
    "                    with open(tabfo + '/out/data_blu_'+ fname+'.csv', 'a') as out:\n",
    "                        out.write(x[i])\n",
    "                else:\n",
    "                    with open(tabfo + '/out/data_blu_'+ fname+'.csv', 'a') as out:\n",
    "                        out.write(\",\" + x[i] )\n",
    "        with open(tabfo + '/out/data_blu_'+ fname+'.csv', 'a') as out:\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "    di= der\n",
    "    der=[\"\"]*len(di)\n",
    "\n",
    "    paz=di[0].split(',')\n",
    "\n",
    "\n",
    "    c=[1]*len(paz) \n",
    "\n",
    "    for i in range(1, len(paz)):\n",
    "        if  paz[i] in hm:\n",
    "            c[i]= 2\n",
    "\n",
    "\n",
    "    print(\"Number of samples for each subset:\\n Gray \",c.count(1)-1,\"Red \", c.count(2))\n",
    "\n",
    "            \n",
    "    for n in range(len(di)):\n",
    "        with open(tabfo + '/out/Vdataset'+ fname+'.csv', 'a') as out:\n",
    "            out.write(di[n] + \"\\n\")\n",
    "            \n",
    "    filelist = glob.glob(tabfo + \"/temp/*.csv\")\n",
    "    for f in filelist:\n",
    "        os.remove(f)            \n",
    "\n",
    "\n",
    "    \n",
    "    print(fname + \" dataset complete\")\n",
    "    input(\"Press enter to continue\")    \n",
    "    \n",
    "    return(c[1:len(c)])\n",
    "\n",
    "####\n",
    "##  SampleSelection\n",
    "## This function simply performs the call to the other functions  twice,\n",
    "## once for the Training dataset, once for the Validation one.\n",
    "## Gene expression input files are chosen by the glob function.\n",
    "##\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "def SampleSelection (tabfo):\n",
    "    classes=[]\n",
    "    for file in glob.glob(tabfo+\"/*.txt\"):\n",
    "\n",
    "        phen, lmu, Hmu, blmu, lif = FiltList(tabfo)\n",
    "        os.system('clear')  \n",
    "        classes.append(Filt(phen, lmu, Hmu, blmu, lif, file, tabfo))\n",
    "\n",
    "    return(classes)\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "tabfo=ChooseFolder()\n",
    "\n",
    "print(\"The working folder is \" + tabfo)\n",
    "\n",
    "clasf= SampleSelection(tabfo)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter how many features you want to use: 1000\n",
      "Script is trying to obtain the top 1000 features.\n"
     ]
    }
   ],
   "source": [
    "from pandas import *\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "####\n",
    "##  TopFeatures\n",
    "## This function will perform the T-test and obtain the \n",
    "## mostly differentiate genes between the two subsets\n",
    "## \n",
    "####\n",
    "\n",
    "\n",
    "def TopFeatures(tabfo, ngen):\n",
    "    print(\"Script is trying to obtain the top \"+ str(ngen) +\" features.\" )\n",
    "    mu = pd.read_csv(tabfo + '/out/VdatasetTraining.csv', sep= \",\", index_col=0)\n",
    "\n",
    "    mu = mu.transpose()\n",
    "\n",
    "    indWT = [i for i, x in enumerate(clasf[0]) if x == 1]\n",
    "    indMU = [i-1 for i, x in enumerate(clasf[0]) if x == 2]\n",
    "\n",
    "    tt=ttest_ind(mu.iloc[indWT], mu.iloc[indMU])\n",
    "\n",
    "\n",
    "    mu = mu.transpose()\n",
    "\n",
    "    mu = mu.assign(pvalue=pd.Series(tt[1]).values)\n",
    "    #mu = mu.assign(tvalue=pd.Series(tt[0]).values)\n",
    "    mu = mu.sort_values('pvalue') \n",
    "    \n",
    "    mu.head(n=100).to_csv('GS.csv', index=True, header=True)\n",
    "    \n",
    "    mu.drop(mu.columns[[-1]], axis=1, inplace=True)\n",
    "    mu=mu.head(n=ngen)\n",
    "\n",
    "    return(mu, list(mu.index))\n",
    "\n",
    "\n",
    "    \n",
    "####\n",
    "##  NumGenes\n",
    "## This function lets the user decide the number of features to use \n",
    "## to build the model\n",
    "## \n",
    "####\n",
    "\n",
    "def NumGenes(prompt=\"Please enter how many features you want to use: \"):\n",
    "\n",
    "    while True:\n",
    "        num_str = input(prompt).strip()\n",
    "        if all(c in '0123456789' for c in num_str):\n",
    "            break\n",
    "        else:\n",
    "            sys.exit(\"Please write an integer\")\n",
    "    return int(num_str)\n",
    "\n",
    "\n",
    "ngen = NumGenes()\n",
    "mu, glist = TopFeatures(tabfo, ngen)\n",
    "#mu, glist = TopFeatures2(tabfo, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection \n",
    "\n",
    "Feature selection is a critical step when dealing with machine learning. \n",
    "Using too few features will make the model work poorly, using too many features will make the model overfit to the training data.\n",
    "The test dataset I am working on has almost 50000 different genes, definitely too many.\n",
    "After testing many different approaches I've decided to perform a T-test between the gene expressions found respectively within the Red and the Gray samples. Rows are then reordered using the T test's outpur and only the top different expressed genes are chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Testing the Machine Learning Methods\n",
    "\n",
    "In order to choose the best Machine Learning approach I've tried different methods, using and adapting to my data a snippet of code found in the Scikit-Learn's documentation. This is not part of the pipeline but it was helpful to me to proceed with the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different Machine Learning Methods:\n",
      "\n",
      "Accuracy: 0.82 (+/- 0.06) [Logistic Regression]\n",
      "Accuracy: 0.78 (+/- 0.07) [Random Forest]\n",
      "Accuracy: 0.75 (+/- 0.09) [Naive Bayes]\n",
      "Accuracy: 0.80 (+/- 0.06) [Ensemble]\n",
      "Accuracy: 0.64 (+/- 0.09) [Stochastic Gradient Descent]\n",
      "Accuracy: 0.77 (+/- 0.09) [Decision Tree]\n",
      "Accuracy: 0.80 (+/- 0.07) [Multilayer Perceptron]\n",
      "Accuracy: 0.77 (+/- 0.06) [Nearest Neighbor]\n",
      "Accuracy: 0.78 (+/- 0.07) [Support Vector Machines]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "####\n",
    "##  TestML\n",
    "## This function tries a number of different Machine Learning\n",
    "## approaches and returns their overall accuracies by performing\n",
    "## 10 fold Cross Validations.\n",
    "## \n",
    "####\n",
    "\n",
    "\n",
    "def TestML(mu, clasf):\n",
    "    dataset_array = mu.values\n",
    "    get=np.transpose(dataset_array)\n",
    "\n",
    "    target=clasf[0]\n",
    "\n",
    "\n",
    "    clf1 = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=10, C=1)\n",
    "    clf2 = RandomForestClassifier(criterion=\"entropy\", max_features=\"log2\", n_estimators= 250)\n",
    "    clf3 = GaussianNB()\n",
    "    clf4 = SGDClassifier(alpha= 0.1, class_weight= \"balanced\", max_iter= 1000,  penalty= \"elasticnet\")\n",
    "    clf5 = tree.DecisionTreeClassifier(criterion= 'entropy', max_depth= 5, max_features= 6, min_samples_leaf= 0.1, min_samples_split= 0.5, splitter= \"best\")\n",
    "    clf6 = MLPClassifier(activation= \"tanh\", alpha= 0.001, hidden_layer_sizes= (50, 50, 100), solver='adam')\n",
    "    clf7 = neighbors.KNeighborsClassifier(algorithm= 'auto', leaf_size= 1, n_neighbors= 10, p= 1, weights= 'uniform')\n",
    "    clf8 = svm.SVC(kernel='linear')\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), ('SGD', clf4), ('dt', clf5), \n",
    "                                        ('mlp', clf6), ('nn', clf7), ('svc', clf8)], voting='hard')\n",
    "    print(\"Testing different Machine Learning Methods:\\n\")\n",
    "\n",
    "    for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, eclf], \n",
    "                          ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble', \n",
    "                           'Stochastic Gradient Descent', 'Decision Tree', 'Multilayer Perceptron','Nearest Neighbor', 'Support Vector Machines']):\n",
    "        scores = cross_val_score(clf, get, target, cv=10, scoring='accuracy')\n",
    "\n",
    "\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "        \n",
    "TestML(mu, clasf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and Validation\n",
    "\n",
    "Using the subset of samples and genes selected in the previous steps now I build the actual classification model.\n",
    "I've chosen to use the Logistic Regression approach since in the previous tests it has proven to be the one with the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The script is building the model\n",
      "The script is testing the model on the training data\n",
      "sensitivity (mut)\n",
      " 0.8938547486033519\n",
      "specificity (wt)\n",
      " 0.5354330708661418\n",
      "overall accuracy\n",
      " 0.745098039216\n",
      "10 Fold Cross Validation Accuracies (Training Set):\n",
      "0.870967741935\n",
      "0.774193548387\n",
      "0.709677419355\n",
      "0.838709677419\n",
      "0.677419354839\n",
      "0.709677419355\n",
      "0.774193548387\n",
      "0.833333333333\n",
      "0.9\n",
      "0.793103448276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "####\n",
    "##  ModelBuild\n",
    "## This function builds the model for the classification.\n",
    "## It uses the Logistic Regression since it was the approach with the highest\n",
    "## accuracy. It also performs a 10 fold Cross Validation and prints on screen\n",
    "## the accuracies\n",
    "## \n",
    "####\n",
    "\n",
    "def ModelBuild(mu, clasf):\n",
    "    print(\"The script is building the model\")\n",
    "\n",
    "    dataset_array = mu.values\n",
    "    get=np.transpose(dataset_array)\n",
    "\n",
    "    target=clasf[0]\n",
    "\n",
    "    clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=10, C=1)\n",
    "\n",
    "    print(\"The script is testing the model on the training data\")\n",
    "\n",
    "    wee=[0.70, 0.67, 0.65, 0.68, 0.58, 0.67, 0.60, 0.61, 0.68, 0.48, 0.74, 0.62, 0.53, 0.57, 0.55, 0.65, 0.63, 0.60, 0.67, 0.62, 0.69, 0.64, 0.80, 0.71, 0.65, 0.48, 0.75, 0.68, 0.61, 0.46, 0.66, 0.61, 0.50, 0.64, 0.62, 0.59, 0.55, 0.67, 0.55, 0.58, 0.70, 0.50, 0.59, 0.61, 0.61, 0.41, 0.57, 0.68, 0.61, 0.44, 0.50, 0.60, 0.68, 0.47, 0.61, 0.67, 0.60, 0.57, 0.70, 0.63, 0.53, 0.64, 0.45, 0.54, 0.48, 0.50, 0.66, 0.58, 0.60, 0.65, 0.58, 0.66, 0.58, 0.70, 0.57, 0.49, 0.58, 0.71, 0.54, 0.59, 0.52, 0.58, 0.53, 0.56, 0.58, 0.56, 0.61, 0.50, 0.55, 0.60, 0.57, 0.61, 0.58, 0.63, 0.50, 0.54, 0.60, 0.75, 0.68, 0.40, 0.37, 0.66, 0.76, 0.68, 0.66, 0.38, 0.60, 0.70, 0.72, 0.62, 0.62, 0.62, 0.74, 0.77, 0.47, 0.43, 0.49, 0.50, 0.37, 0.49, 0.48, 0.43, 0.61, 0.40, 0.71, 0.71, 0.77, 0.59, 0.58, 0.56, 0.55, 0.57, 0.40, 0.62, 0.49, 0.61, 0.58, 0.70, 0.36, 0.55, 0.56, 0.40, 0.56, 0.43, 0.47, 0.48, 0.60, 0.42, 0.65, 0.59, 0.83, 0.46, 0.72, 0.61, 0.56, 0.63, 0.57, 0.57, 0.58, 0.69, 0.67, 0.58, 0.76, 0.65, 0.62, 0.69, 0.56, 0.40, 0.53, 0.57, 0.56, 0.70, 0.65, 0.67, 0.83, 0.67, 0.79, 0.69, 0.68, 0.59, 0.52, 0.38, 0.51, 0.50, 0.51, 0.50, 0.61, 0.72, 0.78, 0.67, 0.60, 0.68, 0.60, 0.56, 0.58, 0.41, 0.55, 0.57, 0.58, 0.83, 0.55, 0.59, 0.55, 0.56, 0.49, 0.67, 0.59, 0.67, 0.53, 0.56, 0.63, 0.67, 0.72, 0.60, 0.74, 0.70, 0.63, 0.61, 0.53, 0.70, 0.60, 0.69, 0.73, 0.67, 0.54, 0.65, 0.63, 0.68, 0.78, 0.69, 0.83, 0.73, 0.63, 0.58, 0.79, 0.55, 0.71, 0.60, 0.71, 0.62, 0.75, 0.62, 0.53, 0.52, 0.67, 0.63, 0.73, 0.69, 0.71, 0.70, 0.67, 0.77, 0.68, 0.66, 0.83, 0.61, 0.62, 0.64, 0.84, 0.67, 0.62, 0.60, 0.63, 0.64, 0.67, 0.78, 0.74, 0.66, 0.81, 0.61, 0.54, 0.71, 0.53, 0.75, 0.65, 0.65, 0.64, 0.54, 0.56, 0.63, 0.65, 0.72, 0.60, 0.64, 0.42, 0.68, 0.76, 0.69, 0.85, 0.70, 0.71, 0.45, 0.55, 0.66, 0.58, 0.63, 0.57, 0.64, 0.50, 0.56, 0.64, 0.65, 0.51, 0.67, 0.67, 0.67]    \n",
    "    \n",
    "    clfe = clf.fit(get, target, sample_weight=wee)\n",
    "\n",
    "    selector = SelectPercentile(f_classif, percentile=0.25)\n",
    "    selector.fit(get, target)\n",
    "\n",
    "    clfe = clf.fit(selector.transform(get), target, sample_weight=wee)\n",
    "       \n",
    "    targg=clfe.predict(selector.transform(get[:, :]))\n",
    "    \n",
    "    CM=confusion_matrix(target, targg)\n",
    "    \n",
    "    print(\"sensitivity (mut)\\n\",float(CM[1][1]/(CM[1][1]+CM[1][0])))   \n",
    "    \n",
    "    print(\"specificity (wt)\\n\", float(CM[0][0]/(CM[0][0]+CM[0][1])))\n",
    "    \n",
    "    print(\"overall accuracy\\n\", accuracy_score(target, targg))    \n",
    "    \n",
    "    cv=cross_val_score(clf, get, target, cv=10, scoring='accuracy')\n",
    "\n",
    "\n",
    "    print(\"10 Fold Cross Validation Accuracies (Training Set):\\n\"+\"\\n\".join(map(str,cv)))\n",
    "    \n",
    "    return(clfe)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "clfe = ModelBuild(mu, clasf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wt 107\n",
      "mut 169\n",
      "sensitivity (mut)\n",
      " 0.9526627218934911\n",
      "specificity (wt)\n",
      " 0.4205607476635514\n",
      "overall accuracy\n",
      " 0.746376811594\n",
      "10 Fold Cross Validation Accuracies (Validation Set):\n",
      "0.785714285714\n",
      "0.785714285714\n",
      "0.857142857143\n",
      "0.75\n",
      "0.785714285714\n",
      "0.714285714286\n",
      "0.714285714286\n",
      "0.777777777778\n",
      "0.592592592593\n",
      "0.769230769231\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "##  ValidationSet\n",
    "## This function uses the model to classify the samples\n",
    "## in the validation set and returns the accuracies\n",
    "## from the 10 fold cross validation.\n",
    "## \n",
    "####\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def ValidationSet(glist, clasf, clfe, mu):\n",
    "    muv = pd.read_csv(tabfo + '/out/VdatasetValidation.csv', sep= \",\", index_col=0)\n",
    "\n",
    "    muv = muv.transpose()\n",
    "\n",
    "    muv = muv[glist]\n",
    "\n",
    "    valid_array = muv.values\n",
    "    getv = valid_array\n",
    "\n",
    "    targetv=clasf[1]\n",
    "\n",
    "    target=clasf[0]\n",
    "    \n",
    "    dataset_array = mu.values\n",
    "    get=np.transpose(dataset_array)\n",
    "\n",
    "    selector = SelectPercentile(f_classif, percentile=0.25)\n",
    "    selector.fit(get, target)\n",
    "    \n",
    "    targgv=clfe.predict(selector.transform(getv[:, :]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"wt\", clasf[1].count(1))            \n",
    "    print(\"mut\", clasf[1].count(2)) \n",
    "   \n",
    "\n",
    "    CM=confusion_matrix(targetv, targgv)\n",
    "    \n",
    "    print(\"sensitivity (mut)\\n\",float(CM[1][1]/(CM[1][1]+CM[1][0])))   \n",
    "    \n",
    "    print(\"specificity (wt)\\n\", float(CM[0][0]/(CM[0][0]+CM[0][1])))\n",
    "    \n",
    "    print(\"overall accuracy\\n\", accuracy_score(targetv, targgv)) \n",
    "\n",
    "    cvv=cross_val_score(clfe, getv, targetv, cv=10, scoring='accuracy')\n",
    "\n",
    "    print(\"10 Fold Cross Validation Accuracies (Validation Set):\\n\"+\"\\n\".join(map(str,cvv)))\n",
    "\n",
    "ValidationSet(glist, clasf, clfe, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 400 features per sample; expecting 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e87ebd050d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#targetv=clasf[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0my_predict_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mcalculate_ovr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ovr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcalculate_ovr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 400 features per sample; expecting 1"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "muv = pd.read_csv(tabfo + '/out/VdatasetValidation.csv', sep= \",\", index_col=0)\n",
    "\n",
    "muv = muv.transpose()\n",
    "\n",
    "muv = muv[glist]\n",
    "\n",
    "valid_array = muv.values\n",
    "getv = valid_array\n",
    "targetv=clasf[1]\n",
    "for i in range(len(clasf[1])):\n",
    "    if clasf[1][i]==1: targetv[i]=0\n",
    "    elif clasf[1][i]==2: targetv[i]=1\n",
    "\n",
    "#targetv=clasf[1]\n",
    "\n",
    "y_predict_probabilities = clfe.predict_proba(getv)[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(targetv, y_predict_probabilities)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Validation set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "muv = pd.read_csv(tabfo + '/out/VdatasetTraining.csv', sep= \",\", index_col=0)\n",
    "\n",
    "muv = muv.transpose()\n",
    "\n",
    "muv = muv[glist]\n",
    "\n",
    "valid_array = muv.values\n",
    "getv = valid_array\n",
    "\n",
    "targetv2=[0]*len(clasf[0])\n",
    "\n",
    "for i in range(len(clasf[1])):\n",
    "    if clasf[0][i]==1: targetv2[i]=0\n",
    "    elif clasf[0][i]==2: targetv2[i]=1\n",
    "\n",
    "print(targetv2)\n",
    "#targetv=clasf[1]\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(getv, target)\n",
    "\n",
    "\n",
    "y_predict_probabilities = clfe.predict_proba(selector.transform(getv))[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(targetv2, y_predict_probabilities)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Training set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer mutations with similiar patterns\n",
    "\n",
    "In this last phase we try to get the list of *PIK3CA* mutations with a gene expression pattern similiar to the ones in the hotspots used to define the \"red\" subset of samples. The script uses the logistic regression model previously made to infer these samples and performs a simple parsing of the mutation table to obtain the loci of said *PIK3CA* mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5778247   0.4221753 ]\n",
      " [ 0.36371013  0.63628987]\n",
      " [ 0.40407598  0.59592402]\n",
      " [ 0.40798311  0.59201689]\n",
      " [ 0.40080111  0.59919889]\n",
      " [ 0.40561857  0.59438143]\n",
      " [ 0.43707263  0.56292737]\n",
      " [ 0.35850195  0.64149805]\n",
      " [ 0.46536018  0.53463982]\n",
      " [ 0.37133816  0.62866184]\n",
      " [ 0.36443282  0.63556718]\n",
      " [ 0.38638195  0.61361805]\n",
      " [ 0.54351152  0.45648848]\n",
      " [ 0.3586899   0.6413101 ]\n",
      " [ 0.38205786  0.61794214]\n",
      " [ 0.45959468  0.54040532]\n",
      " [ 0.39921972  0.60078028]\n",
      " [ 0.42628434  0.57371566]\n",
      " [ 0.60855485  0.39144515]\n",
      " [ 0.49072547  0.50927453]\n",
      " [ 0.53295577  0.46704423]\n",
      " [ 0.35811809  0.64188191]\n",
      " [ 0.38625497  0.61374503]\n",
      " [ 0.53161949  0.46838051]\n",
      " [ 0.34985203  0.65014797]\n",
      " [ 0.45321639  0.54678361]\n",
      " [ 0.50811281  0.49188719]\n",
      " [ 0.43928026  0.56071974]\n",
      " [ 0.40004912  0.59995088]\n",
      " [ 0.43379565  0.56620435]\n",
      " [ 0.36864642  0.63135358]\n",
      " [ 0.4167974   0.5832026 ]\n",
      " [ 0.48396281  0.51603719]\n",
      " [ 0.388379    0.611621  ]\n",
      " [ 0.48869183  0.51130817]\n",
      " [ 0.45274957  0.54725043]\n",
      " [ 0.43147422  0.56852578]]\n",
      "[1 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 1 2 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2]\n",
      "['MTS-T1357', 'MTS-T1188', 'MTS-T0838', 'MTS-T0875', 'MTS-T1024', 'MTS-T1493', 'MTS-T1851', 'MTS-T2387', 'MTS-T0815', 'MTS-T1105', 'MTS-T1916', 'MTS-T0830', 'MTS-T0057', 'MTS-T0126', 'MTS-T0743', 'MTS-T1234', 'MTS-T1529', 'MTS-T1791', 'MTS-T1858', 'MTS-T0024', 'MTS-T0585', 'MTS-T0663', 'MTS-T0889', 'MTS-T0952', 'MTS-T1143', 'MTS-T1467', 'MTS-T1693', 'MTS-T1776', 'MTS-T0484', 'MTS-T0552', 'MTS-T1195']\n",
      "[[ 0.32903272  0.67096728]\n",
      " [ 0.39328807  0.60671193]\n",
      " [ 0.39579371  0.60420629]\n",
      " [ 0.38718815  0.61281185]\n",
      " [ 0.34065681  0.65934319]\n",
      " [ 0.54318845  0.45681155]\n",
      " [ 0.37915189  0.62084811]\n",
      " [ 0.58294589  0.41705411]\n",
      " [ 0.43756727  0.56243273]\n",
      " [ 0.40516815  0.59483185]\n",
      " [ 0.41098269  0.58901731]\n",
      " [ 0.34173949  0.65826051]\n",
      " [ 0.36607153  0.63392847]\n",
      " [ 0.41239269  0.58760731]\n",
      " [ 0.38043883  0.61956117]\n",
      " [ 0.5135341   0.4864659 ]\n",
      " [ 0.47268117  0.52731883]\n",
      " [ 0.43064571  0.56935429]\n",
      " [ 0.43363535  0.56636465]\n",
      " [ 0.43886679  0.56113321]\n",
      " [ 0.37487908  0.62512092]\n",
      " [ 0.44211346  0.55788654]\n",
      " [ 0.38521554  0.61478446]\n",
      " [ 0.4770556   0.5229444 ]\n",
      " [ 0.43265358  0.56734642]\n",
      " [ 0.36449764  0.63550236]\n",
      " [ 0.3728346   0.6271654 ]\n",
      " [ 0.3927534   0.6072466 ]\n",
      " [ 0.35998382  0.64001618]\n",
      " [ 0.40336181  0.59663819]\n",
      " [ 0.55555582  0.44444418]\n",
      " [ 0.48899319  0.51100681]]\n",
      "[2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2]\n",
      "['MTS-T2052', 'MTS-T0006', 'MTS-T0022', 'MTS-T0118', 'MTS-T0274', 'MTS-T0580', 'MTS-T0865', 'MTS-T0901', 'MTS-T0922', 'MTS-T0958', 'MTS-T1060', 'MTS-T1075', 'MTS-T1108', 'MTS-T1133', 'MTS-T1163', 'MTS-T1165', 'MTS-T1209', 'MTS-T1231', 'MTS-T1236', 'MTS-T1480', 'MTS-T1557', 'MTS-T1593', 'MTS-T1634', 'MTS-T1657', 'MTS-T1662', 'MTS-T1793', 'MTS-T1802', 'MTS-T1900']\n",
      "List of mutations in the blue subset classified similiar to the hotspots ones:\n",
      "1064\n",
      "320\n",
      "539\n",
      "449\n",
      "446\n",
      "1069\n",
      "104\n",
      "345\n",
      "1065\n",
      "88\n",
      "546\n",
      "420\n",
      "118\n",
      "115\n",
      "108\n",
      "106\n",
      "471\n",
      "93\n",
      "726\n",
      "1049\n",
      "1043\n",
      "107\n",
      "81\n",
      "1017\n",
      "161\n",
      "365\n",
      "448\n",
      "102\n",
      "1044\n",
      "1067\n",
      "414\n",
      "111\n",
      "110\n",
      "727\n",
      "334\n",
      "\n",
      "\n",
      "lista \n",
      "1017\n",
      "102\n",
      "104\n",
      "1043\n",
      "1044\n",
      "1049\n",
      "106\n",
      "1064\n",
      "1065\n",
      "1065\n",
      "1067\n",
      "1067\n",
      "1069\n",
      "107\n",
      "108\n",
      "110\n",
      "110\n",
      "111\n",
      "115\n",
      "118\n",
      "118\n",
      "118\n",
      "161\n",
      "320\n",
      "334\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "345\n",
      "365\n",
      "414\n",
      "420\n",
      "420\n",
      "420\n",
      "446\n",
      "448\n",
      "448\n",
      "449\n",
      "471\n",
      "539\n",
      "546\n",
      "546\n",
      "546\n",
      "546\n",
      "546\n",
      "546\n",
      "546\n",
      "546\n",
      "546\n",
      "546\n",
      "726\n",
      "726\n",
      "727\n",
      "81\n",
      "88\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####\n",
    "##  FindBlue\n",
    "## This function uses the model to classify the samples\n",
    "## in the \"blue\" datasets from both the Training and the Validation sets\n",
    "## then it uses the list of samples classified as \"red\" to\n",
    "## obtain the list of PIK3CA mutations' loci, classified\n",
    "## by the machine learning, similiar to the ones in the hotspots.\n",
    "## \n",
    "####\n",
    "\n",
    "def FindBlue(blc, glist, clfe, file): \n",
    "    blt = pd.read_csv(file, sep= \",\", index_col=0)\n",
    "\n",
    "    blt = blt.transpose()\n",
    "\n",
    "    blt = blt[glist]\n",
    "\n",
    "    bt_array = blt.values\n",
    "    getbt = bt_array\n",
    "\n",
    "    target=clasf[0]\n",
    "    \n",
    "    dataset_array = mu.values\n",
    "    get=np.transpose(dataset_array)\n",
    "    selector = SelectPercentile(f_classif, percentile=0.25)\n",
    "    selector.fit(get, target)\n",
    "    \n",
    "    \n",
    "    targbt=clfe.predict(selector.transform(getbt[:, :]))\n",
    "    print(clfe.predict_proba(selector.transform(getbt[:, :])))\n",
    "\n",
    "    print(targbt)\n",
    "\n",
    "    btMU = [i-1 for i, x in enumerate(targbt) if x == 2]\n",
    "\n",
    "\n",
    "    btn=list(blt.index[btMU])\n",
    "\n",
    "    print(btn)\n",
    "    \n",
    "    try:\n",
    "        muta = open(tabfo +'/mut.csv',\"r\").read().split('\\n')\n",
    "    except FileNotFoundError:\n",
    "        sys.exit(\"Mutation file cannot be found, was it renamed or moved?\")\n",
    "\n",
    "\n",
    "\n",
    "    for n in range (1, len(muta)-1):\n",
    "        x = muta[n].split(',')\n",
    "        x[9]=x[9]\n",
    "        #print(x[1])\n",
    "        if x[9]==\"PIK3CA\" and x[1] in btn:\n",
    "#        if x[9]==\"ERBB2\" and x[1] in btn:\n",
    "            blc.append(x[12])\n",
    "\n",
    "    return(blc)\n",
    "\n",
    "blc=[]            \n",
    "\n",
    "for file in glob.glob(tabfo+\"/out/data_*.csv\"):\n",
    "    FindBlue(blc, glist, clfe, file)\n",
    "\n",
    "\n",
    "print(\"List of mutations in the blue subset classified similiar to the hotspots ones:\\n\" + \"\\n\".join(map(str,list(set(blc)))))\n",
    "\n",
    "print(\"\\n\\nlista \\n\"+ \"\\n\".join(sorted(blc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
